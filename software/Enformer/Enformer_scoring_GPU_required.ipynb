{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec23ca8-feb9-483b-8eb0-127c5ee8ff56",
   "metadata": {},
   "source": [
    "# example how to score seqeunces with Enformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b7c29-054b-4ff4-9720-ed596b890cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import gzip\n",
    "import shutil\n",
    "#import wget\n",
    "import swifter\n",
    "import os\n",
    "\n",
    "from pyfastaq.sequences import file_reader as fasta_reader\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b3c5f2-cad6-4fa4-b31f-3e4bd8396c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl # to be able to load model\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7f6ec8-d572-4bc9-8a84-67b3ac496dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 393_216\n",
    "bases=list(\"ACGT\")\n",
    "model_path = 'https://tfhub.dev/deepmind/enformer/1'\n",
    "\n",
    "PRED_SQ_SIZE = 114_688\n",
    "BIN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e01b45a-6aa5-44cc-8eb3-49ad6a2a511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant functions Enformer\n",
    "\n",
    "def extract_positions(prediction, experiment_ids, annotations, start_position, end_position, \n",
    "                      full=False,\n",
    "                      fixed_point=None):\n",
    "    \"\"\"\n",
    "    Extracts predicted values given the prediction, IDs of the required experiments, and the positions of interest.\n",
    "    \"\"\"\n",
    "    if fixed_point is None:\n",
    "        fixed_point = start_position\n",
    "    to_pad_left = annotations[1][\"to_pad_left\"]\n",
    "    \n",
    "    start_view = fixed_point - to_pad_left\n",
    "    end_view = start_view + SEQUENCE_LENGTH\n",
    "    middle = (start_view + end_view) // 2\n",
    "    pred_start, pred_end = middle - (PRED_SQ_SIZE // 2), middle + (PRED_SQ_SIZE // 2) \n",
    "    \n",
    "    if full: # return full prediction, with start/end annotation\n",
    "        return prediction[0][:, experiment_ids], pred_start, pred_end\n",
    "    \n",
    "    insert_start = item_position(start_position, pred_start, pred_end)[0]\n",
    "    insert_end = item_position(end_position, pred_start, pred_end)[1]\n",
    "    \n",
    "    return prediction[0][insert_start:insert_end, experiment_ids], insert_start*128+pred_start, insert_end*128+pred_start\n",
    "\n",
    "\n",
    "def item_position(position, pred_start, pred_end, pred_size=896):\n",
    "    posi = np.linspace(pred_start, pred_end, pred_size)\n",
    "    snp_upper = np.where(posi >= position)[0].min()\n",
    "    snp_lower = np.where(posi <= position)[0].max()\n",
    "    return (snp_lower, snp_upper)\n",
    "\n",
    "\n",
    "\n",
    "class FastaStringExtractor:\n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),)\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "\n",
    "\n",
    "\n",
    "class Enformer:\n",
    "    def __init__(self, tfhub_url):\n",
    "        self._model = hub.load(tfhub_url).model\n",
    "    def predict_on_batch(self, inputs):\n",
    "        predictions = self._model.predict_on_batch(inputs)\n",
    "        return {k: v.numpy() for k, v in predictions.items()}\n",
    "    @tf.function\n",
    "    def contribution_input_grad(self, input_sequence,\n",
    "                              target_mask, output_head='human'):\n",
    "        input_sequence = input_sequence[tf.newaxis]\n",
    "        target_mask_mass = tf.reduce_sum(target_mask)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_sequence)\n",
    "            prediction = tf.reduce_sum(\n",
    "              target_mask[tf.newaxis] *\n",
    "              self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n",
    "        input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n",
    "        input_grad = tf.squeeze(input_grad, axis=0)\n",
    "        return tf.reduce_sum(input_grad, axis=-1)\n",
    "\n",
    "\n",
    "def one_hot_encode(sequence):\n",
    "    return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "def list_experiments(organism):\n",
    "    \"\"\"\n",
    "    Returns a pandas DataFrame with all available experiments for prediction.\n",
    "    :param organism: \"human\" or \"mouse\"\n",
    "    :return: pandas DataFrame with all available experiments for prediction\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return ...\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba9282-c11f-46c5-afb4-7abdc57cf617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model=Enformer(model_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71ae1a-9431-464b-8251-f9fd378e5889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the input fasta file contains for each modified enhancer 10 slightly shifted versions of the same seqeunce in its larger genomic context\n",
    "\n",
    "file = '/input_path/5p7_library_seqeunces_syn_enh_lib_12bp_for_enformer_left_align_200kb_homL_10_different_shifts_seed1_of128shiftbins_collapsed_2024_09_27.fasta'\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [(entry.id, entry.seq) for entry in fasta_reader(file)], columns=[\"id\", \"seq\"]\n",
    ")\n",
    "df.head()\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "# depending on library either 10,12,14 bp barcodes\n",
    "to_replace10 = \"NNNNNNNNNN\"\n",
    "to_replace12 = \"NNNNNNNNNNNN\"\n",
    "to_replace14 = \"NNNNNNNNNNNNNN\"\n",
    "\n",
    "barcode_replicate_n = 3 # three random barcodes for each genotype and shift\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i in range(barcode_replicate_n):    \n",
    "    bdf = df.copy()\n",
    "    random_barcode = \"\".join(np.random.choice(bases, size=len(to_replace12)))\n",
    "    bdf[\"seq\"] = bdf[\"seq\"].str.replace(to_replace12, random_barcode)\n",
    "    bdf[\"id\"] = bdf[\"id\"] + f\"_bc_{random_barcode}\"\n",
    "    dfs.append(bdf)\n",
    "\n",
    "fdfs = pd.concat(dfs)\n",
    "fdfs.to_csv('outpath/Info_df_Enformerseqs_lib_5p7_3BCs_seed1_10postional_variations_seed1_2024_10_21.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21e947-2bf3-4997-92d5-672259c0b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pred\n",
    "exp_ind = [12, 69, 5110,688]  # heads used for predictions (all GM12878)\n",
    "\n",
    "one_hot = [one_hot_encode(fdfs['seq'].iloc[x]) for x in range(fdfs.shape[0])]\n",
    "\n",
    "pred=[pd.DataFrame(model.predict_on_batch(one_hot[x][np.newaxis])['human'][0][:,exp_ind])   for x in range(len(one_hot))]\n",
    "\n",
    "predk=pd.concat(pred)\n",
    "\n",
    "predk.to_csv('outpath/Pred_Enf_dnase2x_h3k27ac_CAGE_12_69_5110_688_lib_5p7_3BCs_seed1_10postional_variations_seed1_2024_10_21.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3_9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
